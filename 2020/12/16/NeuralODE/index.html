<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+KR:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","version":"8.1.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>
<meta name="description" content="오늘 다룰 논문은 NeurIPS 2018 에서 뜨거운 관심을 받았던 Neural Ordinary Differential Equations 이다. 이미 논문 제목에서 나와있듯이 Ordinary Differential Equations, 상미분방정식에 대해서 다룬다. 기존에 layer 단위로 쌓아서 성능을 높혀왔던 Neural Network 구조의 틀을 부수는">
<meta property="og:type" content="article">
<meta property="og:title" content="[NeuralODE] Neural Ordinary Differential Equations">
<meta property="og:url" content="http://example.com/2020/12/16/NeuralODE/index.html">
<meta property="og:site_name" content="Andu&#39;s Blog">
<meta property="og:description" content="오늘 다룰 논문은 NeurIPS 2018 에서 뜨거운 관심을 받았던 Neural Ordinary Differential Equations 이다. 이미 논문 제목에서 나와있듯이 Ordinary Differential Equations, 상미분방정식에 대해서 다룬다. 기존에 layer 단위로 쌓아서 성능을 높혀왔던 Neural Network 구조의 틀을 부수는">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/deep_neural_network.png">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/residual_unit.png">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/resnet.png">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/resnet_many.png">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ex_table1.jpg">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ex_table2.jpg">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ex_table3.jpg">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ex_table4.jpg">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ex_graph.jpg">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ode_resnet.jpg">
<meta property="og:image" content="http://example.com/2020/12/16/NeuralODE/ode_resnet_loss.jpg">
<meta property="article:published_time" content="2020-12-16T05:55:27.000Z">
<meta property="article:modified_time" content="2020-12-17T06:43:07.125Z">
<meta property="article:author" content="Andu">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/12/16/NeuralODE/deep_neural_network.png">


<link rel="canonical" href="http://example.com/2020/12/16/NeuralODE/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>[NeuralODE] Neural Ordinary Differential Equations | Andu's Blog</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Andu's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#neural-network"><span class="nav-number">1.</span> <span class="nav-text">Neural Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#residual-network"><span class="nav-number">2.</span> <span class="nav-text">Residual Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ordinary-differential-equations"><span class="nav-number">3.</span> <span class="nav-text">Ordinary Differential Equations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#neural-ode"><span class="nav-number">4.</span> <span class="nav-text">Neural ODE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ode-solver"><span class="nav-number">5.</span> <span class="nav-text">ODE solver</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reverse-mode-automatic-differentiation-of-ode-solution"><span class="nav-number">6.</span> <span class="nav-text">Reverse Mode Automatic Differentiation of ODE Solution</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Andu</p>
  <div class="site-description" itemprop="description">나랑 AI 공부 해보실?</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/godtn0" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;godtn0" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:godtn0@gmail.com" title="E-Mail → mailto:godtn0@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/andu.98" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;andu.98" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/16/NeuralODE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andu">
      <meta itemprop="description" content="나랑 AI 공부 해보실?">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [NeuralODE] Neural Ordinary Differential Equations
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-12-16 14:55:27" itemprop="dateCreated datePublished" datetime="2020-12-16T14:55:27+09:00">2020-12-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-17 15:43:07" itemprop="dateModified" datetime="2020-12-17T15:43:07+09:00">2020-12-17</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/12/16/NeuralODE/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/12/16/NeuralODE/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>오늘 다룰 논문은 <strong><em>NeurIPS 2018</em></strong> 에서 뜨거운 관심을 받았던 <strong><em>Neural Ordinary Differential Equations</em></strong> 이다. 이미 논문 제목에서 나와있듯이 Ordinary Differential Equations, 상미분방정식에 대해서 다룬다. 기존에 layer 단위로 쌓아서 성능을 높혀왔던 Neural Network 구조의 틀을 부수는 듯한 굉장히 신선한 접근을 다룬다. 많은 사람들이 해당 논문의 수식에 의해서 중간에 포기를 하는 경우가 많다. 그래서 최대한 많은 사람들이 이 논문을 완벽히 이해할 수 있도록 노력해서 정리해보겠다. <a id="more"></a></p>
<hr />
<h1 id="neural-network">Neural Network</h1>
<p>아주 간단하게 <strong>Neural Network</strong>에 대한 나의 주관적인 생각을 언급하고 넘어가도록 하겠다. <strong><em>(매우 중요)</em></strong> <strong>Neural Network</strong>는 input과 output의 관계 함수이다. Neural Network에는 <span class="math inline">\(Wx\)</span> 와 같은 선형적인 연산과, <span class="math inline">\(relu(x)\)</span>와 같은 비선형적인 연산을 매우 많이 포함하고있다. 즉, 우리는 Neural Network의 parameter를 잘 조정함으로써 원하는 함수를, 그 함수가 어떠한 형태일지라도 근사해낼 수 있는 것이다. (이 말이 모든 문제를 neural network로 해결할 수 있다는 말은 아니다. 합수의 입력과 출력의 관계가 추상적일지라도 명백히 존재한다면 neural network로 그 함수를 근사해낼 수 있다는 말이다.)</p>
<p>즉, Neural Network는 우리가 어떤 입력과 출력의 관계에 대해서 가정을 할 경우 그 가정에 가장 합당한 함수를 근사해내는 것이라고 생각할 수 있다. 예를 들어, "어떤 사진을 입력으로 넣었을 때 감자인지 고구마인지 구분할 수 있을거야"와 같은 가정 말이다.</p>
<h1 id="residual-network">Residual Network</h1>
<p><strong>Neural ODE</strong>의 이해를 돕기 위해서 <strong>resnet</strong> 의 구조를 살펴볼 필요가 있다. <strong>resnet</strong>은 <strong>ImageNet Challenge</strong> 에서 처음 등장했으며, 기존에 layer가 많아질수록 네트워크 하부에 전달되는 gradient가 작아지는 문제로 인해 더이상 깊은 층을 쌓기 힘들었던 <strong>Deep Neural Networks</strong>의 한계를 깨버린 아주 괄목할 만한 성과를 이뤄낸 구조이다. 그 방법은 매우 심플했다. hidden layer의 activation을 다음 hidden layer의 input으로 직접 주입하는 것이 아닌, <strong>변화량</strong>으로 고려한것이다. 아래 그림을 보도록 하자. <img src="/2020/12/16/NeuralODE/deep_neural_network.png" class="" width="500" title="그림 그리는게 젤 힘듦..."> 위 그림은 기존의 Deep Neural Network의 구조이다. 단순히 hidden layer <span class="math inline">\(W_{t}\)</span> 의 output(activation)을 hidden layer <span class="math inline">\(W_{t+1}\)</span>의 input으로 연결하는 구조이다. 여기다가 <strong>resnet</strong>에서는 아래의 Residual Unit 구조를 적용한다.</p>
<img src="/2020/12/16/NeuralODE/residual_unit.png" class="" width="300" title="그림 그리는게 젤 힘듦...">
<p>residual unit을 적용한 <strong>resnet</strong>의 구조는 아래 그림과 같다. <img src="/2020/12/16/NeuralODE/resnet.png" class="" width="500" title="그림 그리는게 젤 힘듦..."></p>
<p>다시 말해, hidden layer <span class="math inline">\(W_{t}\)</span> 의 output(activation)을 <span class="math inline">\(h_{t}\)</span> 에서 <span class="math inline">\(h_{t+1}\)</span>로 가는 변화량으로 보겠다는 것이다. 이를 수식으로 살짝 표현하면 아래와 같이 쓸 수 있다.</p>
<p><span class="math display">\[\begin{equation} h_{t+1} = h_{t} + f(h_{t}, \theta_{t}) \end{equation}\]</span></p>
<p>여기서 위 <strong>resnet</strong>의 Depth가 매우 깊어진다면 어떨까? 그 식은 <span class="math inline">\(h\)</span> 를 <span class="math inline">\(t\)</span>에 대해서 미분한 식과 매우 유사할 것이다. 왜 그런 것인지 아래 식을 통해 납득해보자.</p>
<p><span class="math display">\[\begin{split}
h_{t+1} - h_{t} = f(h_{t}, \theta_{t})\\\\
\frac{h_{t+1} - h_{t}}{(t+1) - t} = f(h_{t}, \theta_{t})\\\\
\end{split}\]</span></p>
<p>여기서 layer가 매우 많아진다는 것은 <span class="math inline">\(h_{t+1}\)</span> 과 <span class="math inline">\(h_{t}\)</span> 사이의 간격이 매우 좁아진다는 것을 의미한다. layer마다의 간격이 매우 좁아지면 아래 그림과 같이 input과 output을 연결하는 hidden layer들이 무수히 많아진다는 것을 뜻하고, 이는 hidden layer의 output(activation)이 <strong>순간변화율에</strong> 가까워진다고 생각할 수 있다.</p>
<img src="/2020/12/16/NeuralODE/resnet_many.png" class="" width="400" title="그림 그리는게 젤 힘듦...">
<p><span class="math display">\[\begin{equation} \frac{dh(t)}{dt} = f(h(t), t, \theta) \end{equation}\]</span> 우리는 이를 위 식(2)와 같이 미분방정식의 형식으로 표현할 수 있다. (사실 위 식 (1)에서 <span class="math inline">\(t+1 \to t\)</span>로 보내준 것이다.) 다시 말해, <strong>resnet</strong>은 우리가 알고 싶은 input과 output의 관계 식에 대한 <strong>미분방정식</strong>을 <strong>discrete</strong>한(이산적인) 형태로 근사한 함수라고 할 수 있다.</p>
<p>여기서 한번 정리하고 넘어가자. 우리는 입력과 출력의 관계인 함수를 근사하고자 <strong>Neural Network</strong>를 이용한다. 그 중에서도 <strong>ResNet</strong>의 구조는 해당 함수를 근사하기 위해 입력과 출력의 관계 함수에 대한 미분방정식을 이산적으로 근사한 형태로 해석할 수 있다. 아직 이해가 가지 않더라도 걱정하지 말자.</p>
<h1 id="ordinary-differential-equations">Ordinary Differential Equations</h1>
<p>위 식(2)와 같은 미분방정식은 특별하게 <strong>Ordinary Differential Equation(ODE), 상미분방정식</strong> 이라고 부른다. 함수를 결정하는 독립적인 변수가 하나인 경우에 이와 같이 부르며, 식 (2)에서는 독립 변수가 <span class="math inline">\(t\)</span>밖에 존재하지 않으므로 <strong>ODE</strong>라고 부르는 것이다.</p>
<p>실제로 많은 자연 현상이 미분방정식의 형태로 표현되는데, 대표적으로 슈뢰딩거의 파동방정식이 그러하다. 하지만 복잡한 형식의 미분방정식은 풀기 어렵고, 미분방정식의 해가 되는 함수의 근사 함수라도 얻고자 하는 시도가 활발하다. 대표적인 방법으로 <strong>Euler Discretization Method</strong>가 있다. (우리 학과는 미적분학이 필수 교과로 포함되어 있지 않기 떄문에 설명하고 넘어가겠다.) 임의로 정한 <span class="math inline">\(\bigtriangleup x\)</span> 에 대해서 initial value로부터 <span class="math inline">\(y(x_{t+1}) = y(x_{t}) + \bigtriangleup x \cdot \frac{dy}{dx}(x_{t})\)</span> 의 방식으로 근사해 나가는 것이다.</p>
<p>아래의 미분방정식을 풀어보자. <span class="math display">\[\displaystyle \frac{dy}{dx} = y\, , \,\,\,\, y(0)=1 \]</span> 사실 위 미분방정식은 해는 <span class="math inline">\(y=e^x\)</span> 라는 것을 우리는 알고있다. 하지만, 여기서는 위와 같은 미분방정식을 대표적인 이산적 근사방법, <strong>Euler Discretization Method</strong>로 어떻게 풀어내는지 보도록 하자. 우리는 아래와 같은 표를 하나 만들어 낼 수 있다. 우리는 함수를 근사하는 표를 하나 만들것이다. 이 표는 <span class="math inline">\(x\)</span> 와 근사된 <span class="math inline">\(y\)</span> 그리고 해당 <span class="math inline">\((x,y)\)</span> 에서의 <span class="math inline">\(\frac{dy}{dx}\)</span> 로 채워나갈것이다. <img src="/2020/12/16/NeuralODE/ex_table1.jpg" class="" width="400" title="그림 그리는게 젤 힘듦..."> 우선 <span class="math inline">\(x=0\)</span> 에서의 initial value로 <span class="math inline">\(y=1\)</span> 이 주어졌으므로 해당 값을 채운다. 그리고 우리는 <span class="math inline">\(\bigtriangleup x=1\)</span> 단위로 <span class="math inline">\(x\)</span>를 옮겨가며 <span class="math inline">\(y\)</span> 와 <span class="math inline">\(\frac{dy}{dx}\)</span>를 채워나갈것이다. 즉, <span class="math inline">\(y(1) = y(0) + 1 \cdot \frac{dy}{dx}(0)\)</span> 으로 근사를 한다. <img src="/2020/12/16/NeuralODE/ex_table2.jpg" class="" width="400" title="그림 그리는게 젤 힘듦..."> <img src="/2020/12/16/NeuralODE/ex_table3.jpg" class="" width="400" title="그림 그리는게 젤 힘듦..."> <img src="/2020/12/16/NeuralODE/ex_table4.jpg" class="" width="400" title="그림 그리는게 젤 힘듦..."> 휴... 다 채웠다. 이제 위 표를 바탕으로 그래프를 그려 실제 함수인 <span class="math inline">\(y=e^x\)</span> 과 얼마나 가까운지 확인해보자. <img src="/2020/12/16/NeuralODE/ex_graph.jpg" class="" width="400" title="그림 그리는게 젤 힘듦..."> 보다시피, initial value와 멀어질 수록 함수의 근사치에 대한 오차는 증가한다.</p>
<p>그래서 이 얘기를 왜 했을까? 우리의 <strong>ResNet</strong> 의 형태를 다시 보자. <span class="math display">\[h_{t+1} = h_{t} + f(h_{t}, \theta_{t})\]</span> 이 식의 형태가 위에서 본 <strong>Euler Discretization Method</strong>와 매우 유사하지 않나? 결국, 하고자 했던 얘기는 <strong>ResNet</strong>은 어떠한 미분방정식의 함수 해를 찾기 위해 <strong>Euler Discretization Method</strong>와 매우 유사한 방법으로써 함수의 근사를 찾는다고 해석할 수 있다. (위에서 했던 얘기를 부연 설명한 것이다.)</p>
<h1 id="neural-ode">Neural ODE</h1>
<p>드디어 우리는 <strong>Neural ODE</strong>에서 하고자 하는 것을 이해할 수 있게 되었다. 우리가 사용하는 Neural Network는 <strong>Ordinary Differential Equations</strong>의 근사 함수 해를 얻어내는 방법으로 볼 수 있었다. 구체적으로, 각 layer 마다의 output을 <span class="math inline">\(\bigtriangleup x=1\)</span>마다의 <strong>Euler Discretization Method</strong>를 통한 함수값의 근사치라고 해석 할 수 있다. <strong>Neural OD</strong>에서는 결국 initial valude <span class="math inline">\(h(0)\)</span>로부터 떨어진 some time <span class="math inline">\(T\)</span> 에서의 함수값을 한번에 계산해내고 싶은 것이다. 우리에게 주어진 아래의 미분방정식을 가지고, <span class="math display">\[\begin{equation} \frac{dh(t)}{dt} = f(h(t), t, \theta), \;\;h(t_{0}) \end{equation}\]</span> 다음과 같이 <span class="math inline">\(h(t_{1})\)</span>의 값을 계산할 수 있다. <span class="math display">\[\begin{equation} h(t_{1}) = h(t_{0}) + \int_{t_{0}}^{t_{1}}f(h(t),t,\theta)dt \end{equation}\]</span> 아래 그림을 보면 좀 더 이해가 갈 것이다. <img src="/2020/12/16/NeuralODE/ode_resnet.jpg" class="" width="400" title="이 그림의 의도를 이해해줘 제발!!"> 그렇다면, <strong>ResNet</strong>과 같이 <span class="math inline">\(\bigtriangleup x\)</span> 의 크기를 <span class="math inline">\(1\)</span> 로 제한을 둘 필요가 있을까? 그리고 모든 layer들이 동일한 함수 <span class="math inline">\(f\)</span> 에 대해서 근사하는데, 각 layer들의 parameter를 서로 달리 할 필요가 있을까? 이제 좀 감이 오는가?</p>
<h1 id="ode-solver">ODE solver</h1>
<p><strong>ODE solver</strong>는 말 그대로 <strong>ODE</strong>를 해결하는 방법론을 말한다. 예를 들면, 위에서 봤던 <strong>Euler Discretization Method</strong>와 같은 method를 이용해서 <strong>ODE</strong>문제를 해결할 경우 우리는 <strong>Euler Discretization Method ODE sovler</strong>를 쓴다고 할 수 있다. 이 논문에서는 <strong>ODE solver</strong>가 핵심적인 역할을 한다. 이 <strong>ODE solver</strong>를 가지고 기존의 우리의 <span class="math inline">\(loss\;function\)</span> 을 표현해보자. 식을 보면 <strong>ODE solver</strong>의 역할이 더 와닿을 것이다. <span class="math display">\[\begin{equation} 
L(\mathbf{z}(t_{1})) = L(\mathbf{z}(t_{0}) + \int_{t_{0}}^{t_{1}}f(\mathbf{z}(t),t,\theta)dt) = L(ODESolver(\mathbf{z}(t_{0}), f, t_{0}, t_{1}, \theta))
\end{equation}\]</span> 당황하지말고 천천히 위 식을 이해해보자. 우선 <span class="math inline">\(\mathbf{z}(t)\)</span>는 위에서 얘기한 <span class="math inline">\(t\)</span> 번째의 time step t에서의 state, hidden layer의 activation이라고 생각하면 된다. 우리는 최종 <span class="math inline">\(scalar\; loss\)</span>를 가지고 각 hidden layer activation에 대한 <span class="math inline">\(loss\)</span> 함수를 생각할 수 있다. 이를 표현한 것이 <span class="math inline">\(L(\mathbf{z}(t_{1}))\)</span> 이다. 우리는 실제로 위의 정적분 식을 계산 할 수 없다. 정적분을 계산하기 위해선 적어도 <span class="math inline">\(f\)</span> 에 대한 부정적분을 알아야 하지만, 만약 부정적분 식을 알고 있다면 Neural Network를 통한 함수의 근사조차 필요 없어질 것이다. 따라서 이를 근사하기 위해서 위에서 보았던 <strong>Euler Discretization Method</strong>와 같은 <strong>ODE solver</strong>를 이용하는 것이다.</p>
<h1 id="reverse-mode-automatic-differentiation-of-ode-solution">Reverse Mode Automatic Differentiation of ODE Solution</h1>
<p>자 그럼 실제로 위 ODE solver를 가지고 계산한 Loss를 어떻게 optimize 할 것인지, backprop 할 것인지를 알아보자. <strong><em>(수식 주의)</em></strong> 제목은 사실 별 건 아니고, "그래서 backprop 어케 하누!"를 말하는 것이다. <img src="/2020/12/16/NeuralODE/ode_resnet_loss.jpg" class="" width="450" title="이 그림의 의도를 이해해줘 제발!!"> 위 그림은 <strong>ResNet</strong>을 가지고 함수를 근사할 경우에 각 <strong>hidden state</strong>마다 <strong>Loss</strong>와 연결되는 듯한 그림이다. 위 그림에서도 보다시피, <strong>ResNet</strong>에서는 time step <span class="math inline">\(t\)</span> 에 대해서 이산적이고 불연속적으로 state가 전파되므로 우리는 <span class="math inline">\(chain\; rule\)</span> 을 통해 모든 hidden state에 대한 gradient를 계산할 수 있었다. 반면, 우리의 <strong>ODE Solver</strong>를 통한 근사는 time step <span class="math inline">\(t\)</span> 에 대해서 연속적이므로 <span class="math inline">\(chain\; rule\)</span> 을 적용하는데 어려움을 겪게 된다. 여기서 <strong>adjoint sensitivity method</strong>를 이용한다.</p>
<p>먼저, hidden state에서의 <span class="math inline">\(loss\)</span> 에 대한 미분을 결정하자. 이를 우리는 <span class="math inline">\(adjoint \; a(t)\)</span> 라고 하자. <span class="math display">\[\begin{equation}
adjoint, \;\;\; a(t) = {\partial L\over\partial \mathbf{z}(t)} 
\end{equation}\]</span> 우리의 <span class="math inline">\(z(t)\)</span> 는 아래 미분방정식에 대한 해이다. <span class="math display">\[\frac{d\mathbf{z}(t)}{d(t)} = f(\mathbf{z}(t), t, \theta)\]</span> 기존의 Neural Nets 에서는 hidden state <span class="math inline">\(h_{t}\)</span> 에 대한 <span class="math inline">\(L\)</span>의 미분을 아래와 같은 <span class="math inline">\(chain \; rule\)</span> 을 적용한 식으로 표현할 수 있었다. <span class="math display">\[\frac{dL}{dh_{t}} = \frac{dL}{dh_{t+1}} \cdot \frac{dh_{t+1}}{dh_{t}}\]</span> <span class="math inline">\(ODE Solver\)</span> 에서 계산된 hidden state는 위와 같이 discret 하지 않고 연속적이다. 위와 마찬가지로 아주 작은 time step <span class="math inline">\(\varepsilon\)</span> 에 대해서 time step <span class="math inline">\(t+\varepsilon\)</span>에서의 미분을 다음과 같이 표현할 수 있다. <span class="math display">\[\begin{equation}
\mathbf{z}(t+\varepsilon) = \int_{t}^{t+\varepsilon}f(\mathbf{z}, t, \theta) dt + \mathbf{z}(t) = T_{\varepsilon}(\mathbf{z}(t, t)
\end{equation}\]</span> <span class="math display">\[\frac{dL}{d\mathbf{z}(t)} = \frac{dL}{d\mathbf{z}(t+\varepsilon)} \cdot \frac{d\mathbf{z}(t+\varepsilon)}{d\mathbf{z}(t)} \;\;or\]</span> <span class="math display">\[\begin{equation}
a(t) = a(t+\varepsilon) \cdot {\partial{T_{\varepsilon}(\mathbf{z}(t), t)} \over \partial{\mathbf{z}(t)}}
\end{equation}\]</span> 여기서 미분의 정의에 의해, <span class="math display">\[\begin{aligned}
\frac{da(t)}{dt} &amp;= \lim_{\varepsilon \to 0+} \frac{a(t+\varepsilon) - a(t)}{\varepsilon} \\
&amp;= \lim_{\varepsilon \to 0+} \frac{a(t+\varepsilon) - a(t+\varepsilon) \cdot {\partial \over \partial{\mathbf{z}(t)}}T_{\varepsilon} (\mathbf{z}(t), t)}{\varepsilon} \\
&amp;= \lim_{\varepsilon \to 0+} \frac{a(t+\varepsilon) - a(t+\varepsilon) \cdot {\partial \over \partial{\mathbf{z}(t)}}(\mathbf{z}(t) + \varepsilon \cdot f(\mathbf{z}(t), t, \theta) + O(\varepsilon^{2})) }{\varepsilon} \\
&amp;= \lim_{\varepsilon \to 0+} \frac{a(t+\varepsilon) - a(t+\varepsilon) \cdot (I + \varepsilon \cdot {\partial{f(\mathbf{z}(t),t,\theta)} \over \partial{\mathbf{z}(t)}} + O(\varepsilon^{2})) }{\varepsilon} \\
&amp;= \lim_{\varepsilon \to 0+} \frac{ -\varepsilon \cdot a(t+\varepsilon) \cdot {\partial{f(\mathbf{z}(t),t,\theta)} \over \partial{\mathbf{z}(t)}} + O(\varepsilon^{2}) }{\varepsilon} \\
&amp;= \lim_{\varepsilon \to 0+} - a(t+\varepsilon) \cdot {\partial{f(\mathbf{z}(t),t,\theta)} \over \partial{\mathbf{z}(t)}} + O(\varepsilon^{2})\\
&amp;= - a(t) \cdot {\partial{f(\mathbf{z}(t),t,\theta)} \over \partial{\mathbf{z}(t)}}
\end{aligned}\]</span></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/12/15/CycleGAN/" rel="prev" title="[CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks">
                  <i class="fa fa-chevron-left"></i> [CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-grin-stars"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




<script>
if (document.querySelectorAll('.pdf-container').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2.2.4/pdfobject.min.js', () => {
    document.querySelectorAll('.pdf-container').forEach(element => {
      PDFObject.embed(element.dataset.target, element, {
        pdfOpenParams: {
          navpanes : 0,
          toolbar  : 0,
          statusbar: 0,
          pagemode : 'thumbs',
          view     : 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height   : element.dataset.height
      });
    });
  }, window.PDFObject);
}
</script>



  





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://andus-blog.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://example.com/2020/12/16/NeuralODE/";
    this.page.identifier = "2020/12/16/NeuralODE/";
    this.page.title = "[NeuralODE] Neural Ordinary Differential Equations";
    };
  NexT.utils.loadComments('#disqus_thread', () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://andus-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
